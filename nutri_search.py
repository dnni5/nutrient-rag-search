# -*- coding: utf-8 -*-
"""nutri_search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mQ3C9wMvkmI_e8WqPj_y-4ST2qwLtCcC

#Dependencias
"""

!pip install streamlit --quiet
!pip install pyngrok --quiet
!pip install haystack-ai --quiet
!pip install mistral-haystack --quiet
!pip install qdrant-haystack --quiet
!pip install sentence-transformers --quiet
!pip install pandas --quiet

"""#LÃ³gica Principal"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app_logic.py
# import os
# from haystack import Pipeline
# from haystack.components.embedders import SentenceTransformersTextEmbedder
# from haystack.components.builders.prompt_builder import PromptBuilder
# from haystack.utils import Secret
# from haystack.dataclasses import ChatMessage
# 
# # Importaciones de Qdrant y Mistral
# from haystack_integrations.document_stores.qdrant import QdrantDocumentStore
# from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever
# from haystack_integrations.components.generators.mistral import MistralChatGenerator
# 
# # ConfiguraciÃ³n Central
# QDRANT_CLOUD_URL = "https://bcdecddc-b97f-41d3-8e9f-8b41b3eab1b1.europe-west3-0.gcp.cloud.qdrant.io:6333"
# QDRANT_COLLECTION = "nutricion_v1"
# EMBEDDING_MODEL = "sentence-transformers/all-mpnet-base-v2"
# LLM_MODEL = "mistral-large-latest"
# 
# from haystack import component
# 
# @component
# class PromptToMessagesConverter:
#     """
#     Convierte un prompt string a una lista de ChatMessage
#     """
#     @component.output_types(messages=list[ChatMessage])
#     def run(self, prompt: str):
#         messages = [ChatMessage.from_user(prompt)]
#         return {"messages": messages}
# 
# def initialize_query_pipeline(qdrant_api_key: str, mistral_api_key: str):
#     """
#     Inicializa y devuelve un pipeline de Haystack listo para hacer consultas.
#     Se conecta a Qdrant Cloud y configura el modelo de lenguaje.
#     """
#     print("Inicializando el pipeline de consulta...")
# 
#     # 1. Conectar a la base de datos de Qdrant Cloud
#     try:
#         document_store = QdrantDocumentStore(
#             url=QDRANT_CLOUD_URL,
#             api_key=Secret.from_token(qdrant_api_key),
#             index=QDRANT_COLLECTION
#         )
#         print(f"ConexiÃ³n exitosa a la colecciÃ³n '{QDRANT_COLLECTION}' en Qdrant Cloud.")
#         print(f"Documentos encontrados: {document_store.count_documents()}")
#     except Exception as e:
#         print(f"Error al conectar con Qdrant: {e}")
#         return None
# 
#     # 2. Configurar los componentes del pipeline
#     try:
#         # Embedder para convertir la pregunta del usuario en un vector
#         text_embedder = SentenceTransformersTextEmbedder(model=EMBEDDING_MODEL)
# 
#         # Retriever para buscar documentos relevantes en Qdrant
#         retriever = QdrantEmbeddingRetriever(document_store=document_store, top_k=5)
# 
#         # Prompt para darle instrucciones al modelo de lenguaje
#         prompt_template = """Eres un asistente experto en nutriciÃ³n y salud. Tu tarea es responder a la pregunta del usuario basÃ¡ndote Ãºnicamente en el contexto proporcionado. SÃ© claro, conciso y no inventes informaciÃ³n. Si la respuesta no se encuentra en el contexto, indica que no tienes informaciÃ³n suficiente.
# 
# Contextos:
# {% for doc in documents %}
# TÃ­tulo: {{ doc.meta.get('Title', 'N/A') }}
# Autores: {{ doc.meta.get('Authors', 'N/A') }}
# AÃ±o: {{ doc.meta.get('Year', 'N/A') }}
# Fragmento: {{ doc.content }}
# {% endfor %}
# 
# Pregunta: {{ query }}
# Respuesta:"""
# 
#         # PromptBuilder con variables requeridas
#         prompt_builder = PromptBuilder(
#             template=prompt_template,
#             required_variables=["documents", "query"]
#         )
# 
#         # Convertidor personalizado
#         converter = PromptToMessagesConverter()
# 
#         # Generador (LLM) que crearÃ¡ la respuesta final
#         llm = MistralChatGenerator(
#             api_key=Secret.from_token(mistral_api_key),
#             model=LLM_MODEL
#         )
# 
#         # 3. Construir el pipeline
#         pipeline = Pipeline()
#         pipeline.add_component("text_embedder", text_embedder)
#         pipeline.add_component("retriever", retriever)
#         pipeline.add_component("prompt_builder", prompt_builder)
#         pipeline.add_component("converter", converter)
#         pipeline.add_component("llm", llm)
# 
#         # 4. Conectar los componentes en orden
#         pipeline.connect("text_embedder.embedding", "retriever.query_embedding")
#         pipeline.connect("retriever.documents", "prompt_builder.documents")
#         pipeline.connect("prompt_builder.prompt", "converter.prompt")
#         pipeline.connect("converter.messages", "llm.messages")
# 
#         print("Pipeline de consulta listo para usarse.")
#         return pipeline
# 
#     except Exception as e:
#         print(f"Error al inicializar el pipeline: {e}")
#         return None
# 
# def execute_query(pipeline: Pipeline, question: str):
#     """
#     Ejecuta una consulta en el pipeline y devuelve la respuesta y las fuentes.
#     """
#     if not pipeline:
#         return "Error: El pipeline no estÃ¡ inicializado.", []
# 
#     print(f"Ejecutando consulta: '{question}'")
# 
#     try:
#         # Ejecutar el pipeline completo
#         result = pipeline.run({
#             "text_embedder": {"text": question},
#             "prompt_builder": {"query": question}
#         })
# 
#         # Extraer la respuesta
#         if "llm" in result and "replies" in result["llm"] and result["llm"]["replies"]:
#             answer = result["llm"]["replies"][0].text
#         else:
#             answer = "El modelo no generÃ³ una respuesta."
# 
#         # Extraer informaciÃ³n de las fuentes para crear tabla
#         source_documents = []
# 
#         # Obtener documentos del retriever
#         try:
#             # Obtener los componentes
#             text_embedder = pipeline.get_component("text_embedder")
#             retriever = pipeline.get_component("retriever")
# 
#             # Crear embedding
#             embed_result = text_embedder.run(text=question)
# 
#             # Buscar documentos
#             retriever_result = retriever.run(
#                 query_embedding=embed_result["embedding"],
#                 top_k=5
#             )
# 
#             documents = retriever_result.get("documents", [])
# 
#             seen_titles = set()
#             for doc in documents:
# 
#                 title = doc.meta.get('Title', 'TÃ­tulo no disponible')
#                 authors = doc.meta.get('Authors', 'N/A')
#                 year = doc.meta.get('Year', 'N/A')
#                 source = doc.meta.get('Source', 'N/A')
#                 link = doc.meta.get('Link', 'N/A')
# 
#                 # Evitar duplicados por tÃ­tulo
#                 if title not in seen_titles:
#                     source_documents.append({
#                         'TÃ­tulo': title,
#                         'Autores': authors,
#                         'AÃ±o': year,
#                         'Fuente': source,
#                         'Link': link
#                     })
#                     seen_titles.add(title)
# 
#         except Exception as doc_error:
#             print(f"No se pudieron obtener las fuentes: {doc_error}")
# 
#         return answer, source_documents
# 
#     except Exception as e:
#         print(f"Error durante la ejecuciÃ³n de la consulta: {e}")
#         return "OcurriÃ³ un error al procesar tu pregunta.", []

"""# Interfaz"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# from app_logic import initialize_query_pipeline, execute_query
# 
# # ConfiguraciÃ³n de la pÃ¡gina
# st.set_page_config(
#     page_title="NutriSearch",
#     page_icon="ðŸ”Ž",
#     layout="wide"
# )
# 
# st.title("ðŸ¤– Buscador Inteligente de NutriciÃ³n y Salud")
# st.markdown("Hola, soy un buscador. Mis respuestas se basan en una colecciÃ³n de artÃ­culos cientÃ­ficos. Â¿En quÃ© puedo ayudarte hoy?")
# 
# # ConfiguraciÃ³n de API Keys
# QDRANT_API_KEY = "QDRANT_API_KEY"
# MISTRAL_API_KEY = "MISTRAL_API_KEY"
# 
# #  Manejo de Estado
# if "pipeline" not in st.session_state:
#     st.session_state.pipeline = None
# 
# # Auto-inicializar el pipeline
# if st.session_state.pipeline is None and QDRANT_API_KEY and MISTRAL_API_KEY:
#     with st.spinner("Inicializando el pipeline automÃ¡ticamente..."):
#         st.session_state.pipeline = initialize_query_pipeline(
#             qdrant_api_key=QDRANT_API_KEY,
#             mistral_api_key=MISTRAL_API_KEY
#         )
#         if st.session_state.pipeline:
#             st.success("âœ… Pipeline inicializado automÃ¡ticamente. Â¡Listo para usar!")
#         else:
#             st.error("âŒ Error al inicializar el pipeline automÃ¡ticamente.")
# 
# # Interfaz de Chat
# if "messages" not in st.session_state:
#     st.session_state.messages = []
# 
# # Mostrar mensajes anteriores
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])
# 
# # Aceptar la pregunta del usuario
# if prompt := st.chat_input("Escribe tu pregunta sobre nutriciÃ³n aquÃ­..."):
#     # Agregar pregunta al historial
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)
# 
#     # Verificar si el pipeline estÃ¡ listo
#     if st.session_state.pipeline is None:
#         with st.chat_message("assistant"):
#             st.warning("âš ï¸ El pipeline no estÃ¡ inicializado.")
#     else:
#         # Generar y mostrar la respuesta
#         with st.chat_message("assistant"):
#             with st.spinner("Buscando en los documentos y generando respuesta..."):
#                 response, sources = execute_query(st.session_state.pipeline, prompt)
#                 st.markdown(response)
# 
#                 # Mostrar tabla de fuentes
#                 if sources and len(sources) > 0:
#                     st.markdown("---")
#                     st.markdown("ðŸ“š **Fuentes consultadas:**")
# 
#                     # Crear DataFrame para la tabla
#                     df = pd.DataFrame(sources)
#                     df_display = df.drop(columns=['Link'])
#                     st.dataframe(df_display, width="stretch")
# 
#                     # Mostrar links
#                     st.markdown("ðŸ”— **Enlaces:**")
#                     for i, row in df.iterrows():
#                         if row['Link'] and row['Link'] != 'N/A':
#                             st.markdown(f"â€¢ [{row['TÃ­tulo'][:50]}...]({row['Link']})")
# 
#                 # Agregar respuesta al historial
#                 full_response = response
#                 if sources:
#                     full_response += "\n\n---\nðŸ“š **Fuentes consultadas:**\n"
#                     for source in sources:
#                         full_response += f"â€¢ {source['TÃ­tulo']} - {source['Autores']} ({source['AÃ±o']})\n"
# 
#                 st.session_state.messages.append({"role": "assistant", "content": full_response})

"""# Iniciar ngrok y Streamlit



"""

from pyngrok import ngrok

# Authtoken de ngrok
NGROK_AUTH_TOKEN = "NGROK_TOKEN"
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Lanza la app de Streamlit en segundo plano y abre un tÃºnel
public_url = ngrok.connect(8501)
print(f"La aplicaciÃ³n estÃ¡ disponible en: {public_url}")
!streamlit run app.py --server.port 8501